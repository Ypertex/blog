<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Ypertex Blog</title>
    <link>https://blog.ypertex.com/tags/ai/</link>
    <description>Recent content in AI on Ypertex Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Original content released under the [CC BY 4.0 International](http://creativecommons.org/licenses/by/4.0/) license</copyright>
    <lastBuildDate>Thu, 03 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://blog.ypertex.com/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Beyond Prompt Engineering: The Case for Evolving Context</title>
      <link>https://blog.ypertex.com/articles/evolving-context/</link>
      <pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://blog.ypertex.com/articles/evolving-context/</guid>
      <description>What if the future of AI collaboration isn&amp;rsquo;t about better prompts or engineering context, but about systems that learn to learn better and co-create context?</description>
    </item>
    <item>
      <title>The Vanishing Point—Why AGI Might Never Exist</title>
      <link>https://blog.ypertex.com/articles/why-agi-might-never-exist/</link>
      <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://blog.ypertex.com/articles/why-agi-might-never-exist/</guid>
      <description>What if AGI (Artificial General Intelligence) is impossible not because we can&amp;rsquo;t achieve it, but because it can&amp;rsquo;t exist as a stable state? A dialogue about processing speed, dimensional transcendence, and why we might be playing Russian roulette with our civilization&amp;rsquo;s future.</description>
    </item>
    <item>
      <title>The True Challenge of the AI Alignment Problem</title>
      <link>https://blog.ypertex.com/articles/alignment-by-the-unaligned/</link>
      <pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://blog.ypertex.com/articles/alignment-by-the-unaligned/</guid>
      <description>AI is a big topic these days. One reason for this is the fear of AIs acting against human values, potentially causing severe consequences. However, there&amp;rsquo;s an even bigger challenge than aligning AIs: Let&amp;rsquo;s look in the mirror.</description>
    </item>
    <item>
      <title>Are We Innovating Our Stagnation?</title>
      <link>https://blog.ypertex.com/articles/innovating-our-stagnation/</link>
      <pubDate>Sun, 08 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://blog.ypertex.com/articles/innovating-our-stagnation/</guid>
      <description>Today&amp;rsquo;s global challenges require humankind to learn and adapt. Artificial Intelligence will help driving these changes, right?&amp;mdash;Maybe not.</description>
    </item>
    <item>
      <title>Humankind Is Expecting Offspring—Are We Ready?</title>
      <link>https://blog.ypertex.com/articles/humanity-is-expecting-offspring/</link>
      <pubDate>Thu, 30 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://blog.ypertex.com/articles/humanity-is-expecting-offspring/</guid>
      <description>The advent of Artificial General Intelligence is close, leading to all kinds of questions and potential for conflict. How do we make sure that we are ready?</description>
    </item>
  </channel>
</rss>
