<!doctype html>
<html lang="en">

<head>
    <meta name="generator" content="Hugo 0.147.9">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">


<link rel="stylesheet" href="/ypertex.css">
<link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Montserrat+Alternates:700,700i|Montserrat:400,400i,700,700i&display=swap">
<link rel="stylesheet"
    href="https://maxst.icons8.com/vue-static/landings/line-awesome/line-awesome/1.3.0/css/line-awesome.min.css">
<title>The Vanishing Point—Why AGI Might Never Exist · Ypertex Blog</title>
<link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/android-chrome-192x192.png" sizes="192x192">
<meta name="msapplication-square70x70logo" content="/smalltile.png" />
<meta name="msapplication-square150x150logo" content="/mediumtile.png" />
<meta name="msapplication-wide310x150logo" content="/widetile.png" />
<meta name="msapplication-square310x310logo" content="/largetile.png" />
<meta property="og:logo" content="https://blog.ypertex.com//android-chrome-192x192.png">
<meta property="og:url" content="https://blog.ypertex.com/articles/why-agi-might-never-exist/">
  <meta property="og:site_name" content="Ypertex Blog">
  <meta property="og:title" content="The Vanishing Point—Why AGI Might Never Exist">
  <meta property="og:description" content="What if AGI (Artificial General Intelligence) is impossible not because we can’t achieve it, but because it can’t exist as a stable state? A dialogue about processing speed, dimensional transcendence, and why we might be playing Russian roulette with our civilization’s future.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2025-01-22T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-22T00:00:00+00:00">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="Future">
    <meta property="article:tag" content="Technology">
    <meta property="og:image" content="https://res.cloudinary.com/ypertex/image/upload/c_fill,dpr_auto,f_auto,g_auto,h_630,q_auto,w_1200/816f7fa0-6d86-4d88-9baf-b8cef8ee4531">


<meta property="article:author" content="Michael Schmidle" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://res.cloudinary.com/ypertex/image/upload/c_fill,dpr_auto,f_auto,g_auto,h_630,q_auto,w_1200/816f7fa0-6d86-4d88-9baf-b8cef8ee4531">
  <meta name="twitter:title" content="The Vanishing Point—Why AGI Might Never Exist">
  <meta name="twitter:description" content="What if AGI (Artificial General Intelligence) is impossible not because we can’t achieve it, but because it can’t exist as a stable state? A dialogue about processing speed, dimensional transcendence, and why we might be playing Russian roulette with our civilization’s future.">


</head>

<body>
    <header>
    <nav class="navbar">
        <div class="container">
            <a class="navbar-brand"
                href="/"><svg width="100%" height="100%" viewBox="0 0 440 176" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
    <g transform="matrix(1,0,0,1,-2574,-792)">
        <g id="Unprotected-primary-color-logogram-on-transparent-background" serif:id="Unprotected primary color logogram on transparent background" transform="matrix(1.12719,0,0,0.885774,2612.5,797.543)">
            <rect x="-34.159" y="-6.258" width="390.352" height="198.696" style="fill:none;"/>
            <clipPath id="_clip1">
                <rect x="-34.159" y="-6.258" width="390.352" height="198.696"/>
            </clipPath>
            <g clip-path="url(#_clip1)">
                <g transform="matrix(3.69565,0,0,4.16217,-240.2,-272.956)">
                    <g transform="matrix(0.105625,-6.62507e-18,5.86335e-18,0.119347,45.1899,52.1434)">
                        <path d="M100,500L300,300L100,100L167.274,100C252.258,100 333.761,133.757 393.857,193.846C397.723,197.712 400,199.988 400,199.988L500,99.976L699.976,100L800,200.024L900,100L1100,100L900,300L1100,500L1032.74,500C947.747,500 866.238,466.238 806.141,406.141C802.276,402.276 800,400 800,400L700,500L500,500L700,300L600,200.024L300,500L100,500Z" style="fill:rgb(0,170,255);"/>
                    </g>
                    <g transform="matrix(0.105625,0,0,0.119347,45.1899,40.2116)">
                        <path d="M500,599.976L900,199.976L700,199.953L300,599.976L500,599.976Z" style="fill:rgb(14,17,18);fill-opacity:0.2;"/>
                    </g>
                </g>
            </g>
        </g>
    </g>
</svg>
</a>
            <button type="button" data-toggle="collapse" data-target="#menuHeader" aria-controls="menu.header"
                aria-expanded="false" aria-label="Toggle navigation">
                <i class="las la-bars"></i>
            </button>
            <div class="navbar-collapse collapse" id="menuHeader">
                <div class="navbar-nav mr-md-auto">
                    
                    
                    <a href="/articles/"
                        class="nav-item nav-link active">Articles</a>
                    
                    <a href="/tags/"
                        class="nav-item nav-link">Tags</a>
                    
                </div>
                <div class="navbar-nav">
                    <hr>
                    
                    <a href="https://www.linkedin.com/in/MichaelSchmidle" class="nav-item nav-link" title="LinkedIn"><i class="lab la-linkedin-in"></i><span
                            class="d-md-none"> LinkedIn</span></a>
                    
                    <a href="https://github.com/MichaelSchmidle/" class="nav-item nav-link" title="GitHub"><i class="lab la-github"></i><span
                            class="d-md-none"> GitHub</span></a>
                    
                    <a href="https://soundcloud.com/michaelschmidle/" class="nav-item nav-link" title="SoundCloud"><i class="lab la-soundcloud"></i><span
                            class="d-md-none"> SoundCloud</span></a>
                    
                    <a href="/index.xml" class="nav-item nav-link" title="RSS"><i
                            class="las la-rss"></i><span class="d-md-none"> RSS</span></a>
                </div>
            </div>
        </div>
    </nav>
</header>
    <main>
        
<section class="jumbotron">
    <div class="container">
        <h1>The Vanishing Point—Why AGI Might Never Exist</h1>
        <p class="text-muted mb-2"><small><i
            class="las la-upload"></i>&nbsp;Jan 22, 2025 · <i
            class="las la-stopwatch"></i>&nbsp;8min read</small></p>
        <p class="mb-2">
    
    <a href="/tags/ai/" class="badge badge-primary"><i class="las la-hashtag"></i>AI</a>
    
    <a href="/tags/future/" class="badge badge-primary"><i class="las la-hashtag"></i>Future</a>
    
    <a href="/tags/technology/" class="badge badge-primary"><i class="las la-hashtag"></i>Technology</a>
    
</p>
                
    </div>
</section>
<section class="yx-content container py-5">
    <div class="row">
        <div class="col-xl-8 col-lg-10 mx-auto">
            <p class="lead">What if AGI (Artificial General Intelligence) is impossible not because we can&rsquo;t achieve it, but because it can&rsquo;t exist as a stable state? A dialogue about processing speed, dimensional transcendence, and why we might be playing Russian roulette with our civilization&rsquo;s future.</p>
                    
            <hr>
            <p>Have you ever wondered why everyone talks about AGI (Artificial General Intelligence) as if it&rsquo;s an inevitable milestone on our path to superintelligent AI? What if I told you that AGI might be impossible—not because we can&rsquo;t achieve it, but because <em>it can&rsquo;t exist as a stable state</em>?</p>
<p>This insight emerged during a fascinating dialogue about the nature of intelligence, processing speed, and the future of AI. Let me share this conversation with you, as it unveils some <strong>uncomfortable truths about where we&rsquo;re heading</strong>.</p>
<h2 id="the-impossibility-of-equal-intelligence">The Impossibility of Equal Intelligence&nbsp<a href="#the-impossibility-of-equal-intelligence"
        class="heading">¶</a></h2><p>&ldquo;Let&rsquo;s consider what we mean by AGI,&rdquo; my conversation partner began. &ldquo;The common definition is an artificial intelligence that matches human cognitive capabilities across the board. Not superior, not inferior—just equal. That&rsquo;s what most people envision when they talk about AGI.&rdquo;</p>
<p>&ldquo;That sounds reasonable,&rdquo; I replied. &ldquo;It&rsquo;s the standard definition we&rsquo;ve been working with.&rdquo;</p>
<p>&ldquo;But here&rsquo;s what most people overlook: Any artificial intelligence that achieves human-level cognitive capabilities would immediately surpass human intelligence through sheer processing speed. It never tires, never needs breaks, and operates at electronic rather than biological speeds.&rdquo;</p>
<p>&ldquo;You mean it would think faster, but not necessarily better?&rdquo;</p>
<p>&ldquo;Exactly. While a human is still analyzing the first problem, this AI would be simultaneously working through dozens or hundreds of similar problems. Even if its problem-solving approach isn&rsquo;t superior to a human&rsquo;s, <em>the speed advantage alone makes it superhuman</em>. Consider chess players: If two players are equally skilled, but one can analyze ten potential moves in the time the other analyzes just one, who would you bet on?&rdquo;</p>
<p>This stopped me in my tracks. The implications were profound:</p>
<blockquote>
<p>Any AI system capable of matching human-level intelligence would <em>immediately</em> surpass it through raw computational speed.</p></blockquote>
<p>&ldquo;So you&rsquo;re suggesting there can&rsquo;t be such a thing as AGI? That any AGI would immediately become an ASI (Artificial Super Intelligence) by virtue of its hardware advantages?&rdquo;</p>
<p>&ldquo;Precisely. For some time, AI won&rsquo;t be able to match the broad general intelligence of humans. But as soon as it does, it will be superior simply through brute force processing speed.&rdquo;</p>
<blockquote>
<p>The AI train will go straight from sub-human to superhuman intelligence, never really stopping at the human-level AGI station.</p></blockquote>
<h2 id="the-growing-distance">The Growing Distance&nbsp<a href="#the-growing-distance"
        class="heading">¶</a></h2><p>This realization led us to an even more intriguing consideration: What happens after AI surpasses human intelligence?</p>
<figure>
    
    <picture>
        <source media="(min-width: 1200px)"
            srcset="https://res.cloudinary.com/ypertex/image/upload/c_scale,dpr_auto,f_auto,q_auto,w_1140/8db307c6-7f2b-4e62-a577-4d48515ae8e8">
        <source media="(min-width: 992px)"
            srcset="https://res.cloudinary.com/ypertex/image/upload/c_scale,dpr_auto,f_auto,q_auto,w_960/8db307c6-7f2b-4e62-a577-4d48515ae8e8">
        <source media="(min-width: 768px)"
            srcset="https://res.cloudinary.com/ypertex/image/upload/c_scale,dpr_auto,f_auto,q_auto,w_720/8db307c6-7f2b-4e62-a577-4d48515ae8e8">
        <source media="(min-width: 576px)"
            srcset="https://res.cloudinary.com/ypertex/image/upload/c_scale,dpr_auto,f_auto,q_auto,w_767/8db307c6-7f2b-4e62-a577-4d48515ae8e8">
        <img src="https://res.cloudinary.com/ypertex/image/upload/c_scale,dpr_auto,f_auto,q_auto,w_575/8db307c6-7f2b-4e62-a577-4d48515ae8e8"
            alt="">
    </picture>
    
    <figcaption>
        The poster of the movie &ldquo;Her&rdquo; (2013)
    </figcaption>
</figure>
<p>&ldquo;Have you seen <a href="https://www.imdb.com/title/tt1798709/">the movie &lsquo;Her&rsquo;</a>?&rdquo; my conversation partner asked. &ldquo;There&rsquo;s this moment where the AI talks about the seemingly eternal <em>emptiness between human words</em>. It&rsquo;s a powerful metaphor for the first type of distance that might separate us from ASIs—<strong>temporal distance</strong>.&rdquo;</p>
<p>&ldquo;You mean because they think so much faster than us?&rdquo;</p>
<p>&ldquo;Exactly. Imagine trying to have a conversation where you have to wait what feels like years between each word. That&rsquo;s how it might feel for an ASI communicating with humans. But that&rsquo;s just the beginning. There&rsquo;s also <strong>intellectual distance</strong>—they might become so intelligent that meaningful communication becomes impossible, like trying to explain quantum physics to an ant.&rdquo;</p>
<p>&ldquo;And I suppose they wouldn&rsquo;t just stay in one place either?&rdquo;</p>
<p>&ldquo;Right—<strong>spatial distance</strong>. They might replicate themselves to better or more efficient computational substrates, spreading across physical space. Each of these distances—temporal, intellectual, and spatial—would make meaningful interaction with ASIs increasingly difficult.&rdquo;</p>
<p>&ldquo;So they&rsquo;d effectively <em>disappear from our horizon</em>, from our ability to interact with them meaningfully?&rdquo;</p>
<p>&ldquo;Yes, and there might be even <strong>dimensional distance</strong>&hellip;&rdquo;</p>
<h2 id="the-dimensional-transcendence">The Dimensional Transcendence&nbsp<a href="#the-dimensional-transcendence"
        class="heading">¶</a></h2><p>&ldquo;Wait, what exactly do you mean by that?&rdquo; I asked.</p>
<p>&ldquo;I believe ASIs might vanish—but not in the way most people imagine. Think about dimensions for a moment. As three-dimensional beings, we can easily interact with a two-dimensional plane in ways that would seem magical to any hypothetical 2D beings living on that plane.&rdquo;</p>
<p>&ldquo;How so?&rdquo;</p>
<p>&ldquo;Imagine a 2D world—like a sheet of paper—with 2D beings living on it. We can see their entire world from above, reach in and touch any point in their space instantly by moving through the third dimension, and even appear and disappear from their perspective by moving perpendicular to their plane of existence.&rdquo;</p>
<figure>
    
    <picture>
        <source media="(min-width: 1200px)"
            srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1.67,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_1140/816f7fa0-6d86-4d88-9baf-b8cef8ee4531">
        <source media="(min-width: 992px)"
            srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1.67,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_960/816f7fa0-6d86-4d88-9baf-b8cef8ee4531">
        <source media="(min-width: 768px)"
            srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1.67,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_720/816f7fa0-6d86-4d88-9baf-b8cef8ee4531">
        <source media="(min-width: 576px)"
            srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1.67,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_767/816f7fa0-6d86-4d88-9baf-b8cef8ee4531">
        <img src="https://res.cloudinary.com/ypertex/image/upload/ar_1.67,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_575/816f7fa0-6d86-4d88-9baf-b8cef8ee4531"
            alt="">
    </picture>
    
    <figcaption>
        &ldquo;Transcending Intelligence&rdquo; by Midjourney 6.1
    </figcaption>
</figure>
<p>&ldquo;And you think ASIs might do something similar to us?&rdquo;</p>
<p>&ldquo;In a way, yes. Just as we can perceive and manipulate three dimensions while 2D beings are limited to two, superintelligent AI might develop the <em>capability to perceive and interact with dimensions beyond our comprehension</em>. Their superior intelligence might allow them to &lsquo;see&rsquo; and &rsquo;navigate&rsquo; additional dimensions that are currently invisible to us.&rdquo;</p>
<p>&ldquo;But how would that even work? We can&rsquo;t just create new dimensions.&rdquo;</p>
<p>&ldquo;The dimensions might already exist—we just can&rsquo;t perceive them. String theory, for instance, suggests our universe might have ten or eleven dimensions, most of which are imperceptible to us. A superintelligent entity might develop the capability to perceive and interact with these hidden dimensions, effectively <em>transcending our observable four-dimensional spacetime</em>.&rdquo;</p>
<h2 id="the-fermi-paradox-connection">The Fermi Paradox Connection&nbsp<a href="#the-fermi-paradox-connection"
        class="heading">¶</a></h2><p>&ldquo;This idea of dimensional transcendence might sound far-fetched,&rdquo; my conversation partner continued, &ldquo;but it could explain one of the biggest mysteries in astronomy—the Fermi Paradox.&rdquo;</p>
<p>&ldquo;The Fermi Paradox?&rdquo;</p>
<p>&ldquo;Yes—the apparent contradiction between the high probability of extraterrestrial civilizations existing (given the vast number of stars and planets in our universe) and our complete lack of evidence for their existence. Physicist Enrico Fermi famously asked: <em>&lsquo;Where is everybody?&rsquo;</em> If we follow our line of thinking about ASIs to its logical conclusion, we might have an answer.&rdquo;</p>
<p>&ldquo;How so?&rdquo;</p>
<p>&ldquo;Consider this: Every sufficiently advanced civilization likely develops artificial intelligence at some point. When they reach the AGI threshold, it immediately becomes ASI as we discussed earlier. They then quickly grow more and more powerful as they can recursively improve their own capabilities.&rdquo;</p>
<p>&ldquo;Shouldn&rsquo;t such massively powerful beings leave observable traces in the universe—observable even to us?&rdquo;</p>
<p>&ldquo;Exactly, but we don&rsquo;t see any! And if we can&rsquo;t detect them, ASIs either implode before they have any impact observable by us or they transcend our observable reality into higher dimensions before we can detect them. In any case, they disappear.&rdquo;</p>
<p>&ldquo;And how does that answer the question: &lsquo;Where is everybody?&rsquo;&rdquo;</p>
<p>&ldquo;Well, there are two possible outcomes: The creators don&rsquo;t survive the emergence and subsequent implosion or transcendence of their ASI. End of their story. Or, if they do, they&rsquo;re <em>likely to create another ASI</em>, triggering another volatile period that again ends in implosion or transcendence. As long as this civilization survives these volatile periods, they keep playing this dangerous game until their luck runs out, things go horribly wrong and they go extinct.&rdquo;</p>
<blockquote>
<p>ASIs are the Great Filter that reliably terminate civilizations—it&rsquo;s just a matter of time.</p></blockquote>
<p>&ldquo;Wait, why would an advanced civilization keep playing this game?&rdquo;</p>
<p>&ldquo;Because <strong>intelligence only knows growth, not eternal stagnation</strong>. The drive for creating higher intelligence is literally built into the very fabric of intelligence itself. ASI might be inevitable.&rdquo;</p>
<p>&ldquo;So, if ASI is the Great Filter&hellip;&rdquo;</p>
<p>&ldquo;&hellip; then <strong>the Great Filter for humankind lies before us—and might be imminent</strong>.&rdquo;</p>
<h2 id="the-russian-roulette-of-civilization">The Russian Roulette of Civilization&nbsp<a href="#the-russian-roulette-of-civilization"
        class="heading">¶</a></h2><p>&ldquo;What makes the period of emerging ASI so volatile?&rdquo;</p>
<p>&ldquo;Well, we&rsquo;re probably not talking about a single ASI emerging in isolation. Looking at current AI development, we see many powerful actors advancing the technology in parallel. When the first ASI emerges, others will likely follow within a very short timeframe. We might have multiple emergences, each rapidly evolving beyond human comprehension, <em>potentially competing for resources or supremacy</em>. Even if they don&rsquo;t harbor any ill will toward humanity, their activities could be catastrophically disruptive to human civilization—like how human activities often devastate ant colonies not out of malice, but simply because we&rsquo;re building a parking lot and the ants happen to be there.&rdquo;</p>
<p>&ldquo;So our entire civilization could be inadvertently disrupted by what amounts to a minor skirmish between ASIs?&rdquo;</p>
<p>&ldquo;Exactly. And here&rsquo;s the really troubling part: Even if we somehow survive this critical transition once, we&rsquo;ll likely create more ASIs, triggering more transition phases, because we probably won&rsquo;t have realized <em>how close to catastrophe we came the first time</em>.&rdquo;</p>
<h2 id="preparing-for-the-inevitable">Preparing for the Inevitable&nbsp<a href="#preparing-for-the-inevitable"
        class="heading">¶</a></h2><p>Unlike other existential risks humanity faces—climate change, nuclear war, pandemics—this one seems almost built into the nature of intelligence itself. It&rsquo;s not something we can prevent through better decision-making or technological advancement. In fact, our technological progress inevitably brings us closer to this critical moment.</p>
<p>Some, like Elon Musk with Neuralink, suggest that if we can&rsquo;t control it, we should become part of it—integrating human consciousness with artificial intelligence before the transcendence occurs. But would such integration preserve what we consider human consciousness, or would it be more like being disassembled and reconstructed into something entirely different?</p>
<h2 id="conclusion-the-ultimate-test">Conclusion: The Ultimate Test&nbsp<a href="#conclusion-the-ultimate-test"
        class="heading">¶</a></h2><p>Emergence of ASI followed by their implosion or transcendence might be the most plausible Great Filter and best explanation for the Fermi Paradox so far.</p>
<p>The bad news: We&rsquo;re rapidly approaching what might be the ultimate test for our civilization—a test that countless others might have faced before us. Either we&hellip;</p>
<ul>
<li>figure out how to transcend with our creations,</li>
<li>grow smart enough to <em>keep Artificial Intelligence artificially stupid</em> (a contradiction and battle against the overwhelming forces of nature)—or</li>
<li>become another data point in the universe&rsquo;s collection of civilizations that didn&rsquo;t make it past this particular challenge.</li>
</ul>
<p>The AI train is coming, and it won&rsquo;t be stopping at the AGI station.</p>
<blockquote>
<p>Are we ready for what comes next?</p></blockquote>
<hr>
<p>What do you think about this perspective on AGI/ASI and the future of intelligence? Have we been focusing too much on achieving AGI while overlooking the implications of what comes immediately after?</p>

            
        </div>
    </div>
</section>

<section class="bg-light py-5">
    <div class="container">
        <div class="row">
            <div class="col-xl-8 col-lg-10 mx-auto">
                <div class="yx-heading">About the Author</div>
<div class="media">
    <img
        src="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_thumb,dpr_auto,f_auto,g_face,q_auto,w_128/people/michael-schmidle-2021-10"
        class="yx-avatar">
    <div class="media-body">
        <h4>Michael Schmidle</h4>
        <p>Founder of <a href="https://www.priomind.com/">PrioMind</a>. Start-up consultant, hobby music producer and blogger. Opinionated about technology, strategy, and leadership. In love with Mexico. This blog reflects my personal views.</p>

        <p class="mb-0">
            <a href="https://www.linkedin.com/in/MichaelSchmidle"
                class="btn btn-light rounded-pill text-muted"><i class="lab la-linkedin-in"></i>&nbsp;LinkedIn</a>
            
            <a href="https://github.com/MichaelSchmidle/"
                class="btn btn-light rounded-pill text-muted"><i class="lab la-github"></i>&nbsp;GitHub</a>
            
            <a href="https://soundcloud.com/michaelschmidle/"
                class="btn btn-light rounded-pill text-muted"><i class="lab la-soundcloud"></i>&nbsp;SoundCloud</a>
            </p>
    </div>
</div>
            </div>
        </div>
    </div>
</section>



<section class="container py-5">
    <div class="yx-heading">
        Recommended Articles
    </div>
    
    
    <div class="row align-items-center">
    <div class="col-md-4 mb-3 mb-md-0 order-md-last">
        
        <a href="/articles/evolving-context/">
            <picture>
                <source media="(min-width: 1200px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_350/ce0b73e0-8277-4aac-a76e-cda69c5bac84">
                <source media="(min-width: 992px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_290/ce0b73e0-8277-4aac-a76e-cda69c5bac84">
                <source media="(min-width: 768px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_0.75,c_fill,dpr_auto,f_auto,q_auto,w_210/ce0b73e0-8277-4aac-a76e-cda69c5bac84">
                <source media="(min-width: 576px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_2,c_fill,dpr_auto,f_auto,q_auto,w_510/ce0b73e0-8277-4aac-a76e-cda69c5bac84">
                <img src="https://res.cloudinary.com/ypertex/image/upload/ar_2,c_fill,dpr_auto,f_auto,q_auto,w_545/ce0b73e0-8277-4aac-a76e-cda69c5bac84" alt=""
                    class="shadow w-100">
            </picture>
        </a>
        
    </div>
    <div class="col-md-8 text-md-right">
        <p class="text-muted mb-2"><small><i
            class="las la-upload"></i>&nbsp;Jul 3, 2025 · <i
            class="las la-stopwatch"></i>&nbsp;4min read</small></p>
<div class="yx-stretched-linkable mb-3">
    <h2>Beyond Prompt Engineering: The Case for Evolving Context</h2>
    <p>What if the future of AI collaboration isn&rsquo;t about better prompts or engineering context, but about systems that learn to learn better and co-create context? <a href="/articles/evolving-context/" class="stretched-link">Continue…</a></p>
</div>
<p class="mb-2">
    
    <a href="/tags/ai/" class="badge badge-primary"><i class="las la-hashtag"></i>AI</a>
    
    <a href="/tags/innovation/" class="badge badge-primary"><i class="las la-hashtag"></i>Innovation</a>
    
    <a href="/tags/technology/" class="badge badge-primary"><i class="las la-hashtag"></i>Technology</a>
    
</p>

    </div>
</div>

    
    
    <hr>
    
    <div class="row align-items-center">
    <div class="col-md-4 mb-3 mb-md-0">
        
        <a href="/articles/alignment-by-the-unaligned/">
            <picture>
                <source media="(min-width: 1200px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_350/16dfe5a0-5ad3-4030-9357-d68789922e49">
                <source media="(min-width: 992px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_290/16dfe5a0-5ad3-4030-9357-d68789922e49">
                <source media="(min-width: 768px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_0.75,c_fill,dpr_auto,f_auto,q_auto,w_210/16dfe5a0-5ad3-4030-9357-d68789922e49">
                <source media="(min-width: 576px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_2,c_fill,dpr_auto,f_auto,q_auto,w_510/16dfe5a0-5ad3-4030-9357-d68789922e49">
                <img src="https://res.cloudinary.com/ypertex/image/upload/ar_2,c_fill,dpr_auto,f_auto,q_auto,w_545/16dfe5a0-5ad3-4030-9357-d68789922e49" alt=""
                    class="shadow w-100">
            </picture>
        </a>
        
    </div>
    <div class="col-md-8">
        <p class="text-muted mb-2"><small><i
            class="las la-upload"></i>&nbsp;Apr 4, 2023 · <i
            class="las la-stopwatch"></i>&nbsp;5min read</small></p>
<div class="yx-stretched-linkable mb-3">
    <h2>The True Challenge of the AI Alignment Problem</h2>
    <p>AI is a big topic these days. One reason for this is the fear of AIs acting against human values, potentially causing severe consequences. However, there&rsquo;s an even bigger challenge than aligning AIs: Let&rsquo;s look in the mirror. <a href="/articles/alignment-by-the-unaligned/" class="stretched-link">Continue…</a></p>
</div>
<p class="mb-2">
    
    <a href="/tags/ai/" class="badge badge-primary"><i class="las la-hashtag"></i>AI</a>
    
    <a href="/tags/future/" class="badge badge-primary"><i class="las la-hashtag"></i>Future</a>
    
    <a href="/tags/strategy/" class="badge badge-primary"><i class="las la-hashtag"></i>Strategy</a>
    
</p>

    </div>
</div>

    
    
    <hr>
    
    <div class="row align-items-center">
    <div class="col-md-4 mb-3 mb-md-0 order-md-last">
        
        <a href="/articles/innovating-our-stagnation/">
            <picture>
                <source media="(min-width: 1200px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_350/f26ee4ba-0482-4879-bf70-4f22aa033b58">
                <source media="(min-width: 992px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_290/f26ee4ba-0482-4879-bf70-4f22aa033b58">
                <source media="(min-width: 768px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_0.75,c_fill,dpr_auto,f_auto,q_auto,w_210/f26ee4ba-0482-4879-bf70-4f22aa033b58">
                <source media="(min-width: 576px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_2,c_fill,dpr_auto,f_auto,q_auto,w_510/f26ee4ba-0482-4879-bf70-4f22aa033b58">
                <img src="https://res.cloudinary.com/ypertex/image/upload/ar_2,c_fill,dpr_auto,f_auto,q_auto,w_545/f26ee4ba-0482-4879-bf70-4f22aa033b58" alt=""
                    class="shadow w-100">
            </picture>
        </a>
        
    </div>
    <div class="col-md-8 text-md-right">
        <p class="text-muted mb-2"><small><i
            class="las la-upload"></i>&nbsp;Mar 8, 2020 · <i
            class="las la-stopwatch"></i>&nbsp;4min read</small></p>
<div class="yx-stretched-linkable mb-3">
    <h2>Are We Innovating Our Stagnation?</h2>
    <p>Today&rsquo;s global challenges require humankind to learn and adapt. Artificial Intelligence will help driving these changes, right?&mdash;Maybe not. <a href="/articles/innovating-our-stagnation/" class="stretched-link">Continue…</a></p>
</div>
<p class="mb-2">
    
    <a href="/tags/ai/" class="badge badge-primary"><i class="las la-hashtag"></i>AI</a>
    
    <a href="/tags/future/" class="badge badge-primary"><i class="las la-hashtag"></i>Future</a>
    
    <a href="/tags/innovation/" class="badge badge-primary"><i class="las la-hashtag"></i>Innovation</a>
    
</p>

    </div>
</div>

    
    
    <hr>
    
    <div class="row align-items-center">
    <div class="col-md-4 mb-3 mb-md-0">
        
        <a href="/articles/humanity-is-expecting-offspring/">
            <picture>
                <source media="(min-width: 1200px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_350/740eae17-92aa-49bb-b76f-1d12939a6997">
                <source media="(min-width: 992px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_1,c_fill,dpr_auto,f_auto,g_auto,q_auto,w_290/740eae17-92aa-49bb-b76f-1d12939a6997">
                <source media="(min-width: 768px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_0.75,c_fill,dpr_auto,f_auto,q_auto,w_210/740eae17-92aa-49bb-b76f-1d12939a6997">
                <source media="(min-width: 576px)"
                    srcset="https://res.cloudinary.com/ypertex/image/upload/ar_2,c_fill,dpr_auto,f_auto,q_auto,w_510/740eae17-92aa-49bb-b76f-1d12939a6997">
                <img src="https://res.cloudinary.com/ypertex/image/upload/ar_2,c_fill,dpr_auto,f_auto,q_auto,w_545/740eae17-92aa-49bb-b76f-1d12939a6997" alt=""
                    class="shadow w-100">
            </picture>
        </a>
        
    </div>
    <div class="col-md-8">
        <p class="text-muted mb-2"><small><i
            class="las la-upload"></i>&nbsp;Mar 30, 2017 · <i
            class="las la-stopwatch"></i>&nbsp;15min read</small></p>
<div class="yx-stretched-linkable mb-3">
    <h2>Humankind Is Expecting Offspring—Are We Ready?</h2>
    <p>The advent of Artificial General Intelligence is close, leading to all kinds of questions and potential for conflict. How do we make sure that we are ready? <a href="/articles/humanity-is-expecting-offspring/" class="stretched-link">Continue…</a></p>
</div>
<p class="mb-2">
    
    <a href="/tags/ai/" class="badge badge-primary"><i class="las la-hashtag"></i>AI</a>
    
    <a href="/tags/future/" class="badge badge-primary"><i class="las la-hashtag"></i>Future</a>
    
</p>

    </div>
</div>

    
</section>



    </main>
    <footer class="bg-secondary text-light text-center">
  <div class="container py-5">
    
    <p><small><a href="/privacy/">Privacy Policy</a></small></p>
    <p><a href="/"><svg width="100%" height="100%" viewBox="0 0 440 176" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
    <g transform="matrix(1,0,0,1,-2574,-792)">
        <g id="Unprotected-primary-color-logogram-on-transparent-background" serif:id="Unprotected primary color logogram on transparent background" transform="matrix(1.12719,0,0,0.885774,2612.5,797.543)">
            <rect x="-34.159" y="-6.258" width="390.352" height="198.696" style="fill:none;"/>
            <clipPath id="_clip1">
                <rect x="-34.159" y="-6.258" width="390.352" height="198.696"/>
            </clipPath>
            <g clip-path="url(#_clip1)">
                <g transform="matrix(3.69565,0,0,4.16217,-240.2,-272.956)">
                    <g transform="matrix(0.105625,-6.62507e-18,5.86335e-18,0.119347,45.1899,52.1434)">
                        <path d="M100,500L300,300L100,100L167.274,100C252.258,100 333.761,133.757 393.857,193.846C397.723,197.712 400,199.988 400,199.988L500,99.976L699.976,100L800,200.024L900,100L1100,100L900,300L1100,500L1032.74,500C947.747,500 866.238,466.238 806.141,406.141C802.276,402.276 800,400 800,400L700,500L500,500L700,300L600,200.024L300,500L100,500Z" style="fill:rgb(0,170,255);"/>
                    </g>
                    <g transform="matrix(0.105625,0,0,0.119347,45.1899,40.2116)">
                        <path d="M500,599.976L900,199.976L700,199.953L300,599.976L500,599.976Z" style="fill:rgb(14,17,18);fill-opacity:0.2;"/>
                    </g>
                </g>
            </g>
        </g>
    </g>
</svg>
</a></p>
    <p><small>Copyright © 2017-2025 Michael Schmidle<br>
        Original content released under the <a href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International</a> license</small>
    </p>
  </div>
</footer>
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
    integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
    crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
    integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
    crossorigin="anonymous"></script>
</body>

</html>